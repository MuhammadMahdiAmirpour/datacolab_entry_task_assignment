{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Mounting the google drive**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "wwhE9EJp52jF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# you need to uncomment this line when you want to run this code on google colab and load you data from google drive\n",
        "from google.colab import drive\n",
        "# Mount Google Drive as a local file system\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "qe3gUvLo5ojv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importing the needed libraries**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "TVSIFUqT6KoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration"
      ],
      "metadata": {
        "id": "9EEFgYHJ5j4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Setting up the environment**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "nzsUh66D6pOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the CUDA allocation configuration to use expandable segments\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "\n",
        "# Define global variables for input and output file paths, you can change this if you have your custom dataset\n",
        "INPUT_FILE_PATH = '/content/drive/MyDrive/datacolab_dataset/booksummaries.txt'\n",
        "OUTPUT_FILE_PATH = '/content/drive/MyDrive/datacolab_dataset/summary_outputs/output.csv'"
      ],
      "metadata": {
        "id": "cvp53gKs5uN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Using parallelization and batch processing to summarize the texts**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "QoQoKFi_6xON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class T5SummarizationPipeline:\n",
        "    def __init__(self, model_name: str, device: str):\n",
        "        self.model_name = model_name\n",
        "        self.device = device\n",
        "        self.tokenizer = T5Tokenizer.from_pretrained(self.model_name, legacy=False, use_fast=True)\n",
        "        self.model = T5ForConditionalGeneration.from_pretrained(self.model_name).to(self.device)\n",
        "        self.model = self.model.half()  # Enable mixed precision training\n",
        "\n",
        "    def summarize_batch(self, input_texts: list[str]) -> list[str]:\n",
        "        inputs = self.tokenizer.batch_encode_plus([\"summarize: \" + text for text in input_texts], return_tensors=\"pt\",\n",
        "                                                  max_length=1024, truncation=True, padding=True).to(self.model.device)\n",
        "        with torch.cuda.stream(torch.cuda.Stream()):\n",
        "            summary_ids = self.model.generate(**inputs, max_length=150, min_length=50, length_penalty=2.0, num_beams=4,\n",
        "                                              early_stopping=True)\n",
        "        summarized_texts = [self.tokenizer.decode(ids, skip_special_tokens=True) for ids in summary_ids]\n",
        "        return summarized_texts\n",
        "\n",
        "\n",
        "def summarize_summaries_parallel(pipeline: T5SummarizationPipeline, df: pd.DataFrame, batch_size: int,\n",
        "                                 output_file: str) -> None:\n",
        "    # Open the output file in append mode\n",
        "    with open(output_file, 'a') as f:\n",
        "        # Check if the output file is empty\n",
        "        if os.stat(output_file).st_size == 0:\n",
        "            # Write the header for the output CSV file\n",
        "            f.write('freebaseID,CondensedSummary\\n')\n",
        "            # Start the summarization process from the beginning\n",
        "            start_index = 0\n",
        "        else:\n",
        "            # Read the existing freebaseIDs from the output CSV file\n",
        "            existing_freebase_id = pd.read_csv(output_file, header=None)[0].tolist()\n",
        "            # Find the index of the first non-condensed summary\n",
        "            start_index = df[~df['freebase_id'].isin(existing_freebase_id)].index[0]\n",
        "\n",
        "        # Summarize the remaining texts\n",
        "        for i in range(start_index, len(df), batch_size):\n",
        "            batch_texts = df['summary'].tolist()[i:i + batch_size]\n",
        "            batch_summaries = pipeline.summarize_batch(batch_texts)\n",
        "            batch_df = pd.DataFrame(\n",
        "                {'freebaseID': df['freebase_id'].tolist()[i:i + batch_size], 'CondensedSummary': batch_summaries})\n",
        "\n",
        "            # Write the batch to the output CSV file\n",
        "            batch_df.to_csv(f, index=False, header=False)\n",
        "\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        # Sample DataFrame with a 'summary' column\n",
        "        column_names = [\"length\", \"freebase_id\", \"book_name\", \"author_name\", \"date\", \"freebase_id_json\", \"summary\"]\n",
        "        data = pd.read_csv(INPUT_FILE_PATH, sep=\"\\t\", header=None, names=column_names)\n",
        "        df = pd.DataFrame(data)\n",
        "\n",
        "        # Set the output file path\n",
        "        output_file = OUTPUT_FILE_PATH\n",
        "\n",
        "        # Check if the output file exists\n",
        "        if os.path.exists(output_file):\n",
        "            # Read the existing freebaseIDs from the output CSV file\n",
        "            existing_freebase_id = pd.read_csv(output_file, header=None)[0].tolist()\n",
        "            # Check if all the freebaseIDs are already in the output CSV file\n",
        "            if set(df['freebase_id']).issubset(existing_freebase_id):\n",
        "                print(\"All summaries are condensed.\")\n",
        "                return\n",
        "\n",
        "        # Apply parallelized summarization to the DataFrame\n",
        "        pipeline = T5SummarizationPipeline(model_name=\"t5-small\", device='cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        summarize_summaries_parallel(pipeline, df, batch_size=32, output_file=output_file)\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "QunHqBzDzNrY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}